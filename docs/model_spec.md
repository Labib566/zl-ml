1.1 Model Selection Rationale
Objective

The goal of the machine learning model is to compute a binary decision
(e.g., approve / reject) based on a fixed-size feature vector, under the following constraints:

Deterministic inference

Compatibility with arithmetic circuits

Low multiplicative depth

Bounded numeric range

Efficient zero-knowledge proof generation

Given these constraints, we select a linear classification model with a sigmoid decision boundary, equivalent to logistic regression.

Why Logistic Regression?
Criterion	Justification
Determinism	Fully deterministic at inference time
Arithmetic Simplicity	Linear operations + sigmoid approximation
ZK Compatibility	Low-degree polynomial approximation
Explainability	Feature weights are interpretable
Constraint Efficiency	Minimal circuit depth

Note: Training complexity is irrelevant for ZK.
Only inference complexity matters.

1.2 Feature Space Definition

Let the input feature vector be:

ğ‘¥
=
(
ğ‘¥
1
,
ğ‘¥
2
,
â€¦
,
ğ‘¥
ğ‘›
)
âˆˆ
ğ‘…
ğ‘›
x=(x
1
	â€‹

,x
2
	â€‹

,â€¦,x
n
	â€‹

)âˆˆR
n

Where:

ğ‘›
n is fixed at compile time

All features are normalized and bounded

Feature Constraints

Each feature satisfies:

ğ‘¥
ğ‘–
âˆˆ
[
ğ‘¥
min
â¡
,
ğ‘¥
max
â¡
]
x
i
	â€‹

âˆˆ[x
min
	â€‹

,x
max
	â€‹

]

These bounds are enforced during input preprocessing and validated before proof generation.

1.3 Fixed-Point Numeric Representation (CRITICAL)
Motivation

Zero-knowledge circuits do not support floating-point arithmetic.
Therefore, all real-valued computations must be represented using fixed-point integers.

Fixed-Point Encoding

Let:

scaling factor 
ğ‘†
=
2
ğ‘˜
S=2
k
, where 
ğ‘˜
âˆˆ
ğ‘
kâˆˆN

A real value 
ğ‘¥
x is encoded as:

ğ‘¥
^
=
âŒŠ
ğ‘¥
â‹…
ğ‘†
âŒ‰
x
^
=âŒŠxâ‹…SâŒ‰

Similarly, model weights 
ğ‘¤
ğ‘–
w
i
	â€‹

 and bias 
ğ‘
b are encoded as:

ğ‘¤
^
ğ‘–
=
âŒŠ
ğ‘¤
ğ‘–
â‹…
ğ‘†
âŒ‰
,
ğ‘
^
=
âŒŠ
ğ‘
â‹…
ğ‘†
âŒ‰
w
^
i
	â€‹

=âŒŠw
i
	â€‹

â‹…SâŒ‰,
b
^
=âŒŠbâ‹…SâŒ‰

All arithmetic inside the circuit operates on integer values 
ğ‘¥
^
,
ğ‘¤
^
,
ğ‘
^
x
^
,
w
^
,
b
^
.

Overflow Bound Analysis

Let:

ğ‘›
n = number of features

ğ‘€
ğ‘¥
M
x
	â€‹

 = max absolute feature value

ğ‘€
ğ‘¤
M
w
	â€‹

 = max absolute weight value

The maximum linear sum magnitude is:

âˆ£
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¤
^
ğ‘–
â‹…
ğ‘¥
^
ğ‘–
âˆ£
â‰¤
ğ‘›
â‹…
(
ğ‘€
ğ‘¤
ğ‘†
)
â‹…
(
ğ‘€
ğ‘¥
ğ‘†
)
âˆ£
i=1
âˆ‘
n
	â€‹

w
^
i
	â€‹

â‹…
x
^
i
	â€‹

âˆ£â‰¤nâ‹…(M
w
	â€‹

S)â‹…(M
x
	â€‹

S)

This bound must be strictly less than the field modulus of the zkSNARK system.

This bound will later dictate our circuit field choice and scaling factor.

1.4 Formal Inference Equation
Linear Combination

The pre-activation value is computed as:

ğ‘§
=
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¤
ğ‘–
ğ‘¥
ğ‘–
+
ğ‘
z=
i=1
âˆ‘
n
	â€‹

w
i
	â€‹

x
i
	â€‹

+b

In fixed-point form:

ğ‘§
^
=
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¤
^
ğ‘–
â‹…
ğ‘¥
^
ğ‘–
+
ğ‘
^
â‹…
ğ‘†
z
^
=
i=1
âˆ‘
n
	â€‹

w
^
i
	â€‹

â‹…
x
^
i
	â€‹

+
b
^
â‹…S

Note: One additional multiplication by 
ğ‘†
S is introduced to preserve scale consistency.

Sigmoid Approximation

The logistic sigmoid function:

ğœ
(
ğ‘§
)
=
1
1
+
ğ‘’
âˆ’
ğ‘§
Ïƒ(z)=
1+e
âˆ’z
1
	â€‹


is not directly computable inside a ZK circuit.

Therefore, we approximate it using a low-degree polynomial:

ğœ
(
ğ‘§
)
â‰ˆ
ğ‘ƒ
(
ğ‘§
)
=
ğ‘
0
+
ğ‘
1
ğ‘§
+
ğ‘
3
ğ‘§
3
Ïƒ(z)â‰ˆP(z)=a
0
	â€‹

+a
1
	â€‹

z+a
3
	â€‹

z
3

Where coefficients 
ğ‘
0
,
ğ‘
1
,
ğ‘
3
a
0
	â€‹

,a
1
	â€‹

,a
3
	â€‹

 are precomputed constants.

Polynomial degree is chosen to balance:

approximation error

constraint complexity

Final Decision Rule

Let the output probability be:

ğ‘¦
^
=
ğ‘ƒ
(
ğ‘§
^
)
y
^
	â€‹

=P(
z
^
)

The binary decision is defined as:

decision
=
{
1
	
if 
ğ‘¦
^
â‰¥
ğœ


0
	
otherwise
decision={
1
0
	â€‹

if 
y
^
	â€‹

â‰¥Ï„
otherwise
	â€‹


Where:

ğœ
Ï„ is a fixed threshold encoded in fixed-point form.

1.5 Determinism Guarantee

The model guarantees determinism because:

All operations are integer arithmetic

No randomness is used

All parameters are fixed at compile time

Therefore:

âˆ€
ğ‘¥
,
ğ‘“
(
ğ‘¥
)
 produces a unique output
âˆ€x,f(x) produces a unique output

This property is mandatory for sound zero-knowledge verification.

1.6 Public vs Private Values
Component	Visibility
Input features	Private
Model weights	Private
Bias	Private
Threshold 
ğœ
Ï„	Public
Decision output	Public
Zero-knowledge proof	Public

This separation ensures:

model confidentiality,

user privacy,

public verifiability.

1.7 Phase 1 Deliverables (Checklist)

âœ… Formal model selection rationale
âœ… Fixed-point numeric system
âœ… Overflow bounds
âœ… Formal inference equation
âœ… Sigmoid approximation strategy
âœ… Determinism proof

ğŸ‘‰ PHASE 1 COMPLETE